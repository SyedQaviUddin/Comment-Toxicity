## ğŸ›¡ï¸ Deep Learning for Comment Toxicity Detection with Streamlit

# ğŸ“– Project Overview

This project develops a deep learning-based model to automatically detect toxic comments such as hate speech, harassment, or offensive language.
It combines NLP preprocessing, a BiLSTM neural network, and a Streamlit web app for real-time moderation.

The system helps online platforms and communities flag toxic content in real-time, supporting healthier and safer online discussions.

# ğŸ¯ Objectives
Train a deep learning model to classify comments as toxic/non-toxic.
Build an interactive Streamlit app for real-time predictions.
Support bulk predictions via CSV upload.
Provide tools for content moderation in social media, forums, e-learning, and more.

# ğŸ› ï¸ Skills Gained
Deep Learning (BiLSTM, NLP)
Model Training, Evaluation & Optimization
Streamlit Web App Development
Model Deployment

# ğŸ§© Problem Statement
Online communities are plagued by toxic comments.
The goal is to build an automated system to:
Detect toxic comments in real-time.
Assist moderators in filtering, warning, or reviewing harmful content.

# ğŸ”‘ Business Use Cases
Social Media Platforms â†’ Auto-remove toxic comments.
Forums & Communities â†’ Efficient moderation.
Brand Safety â†’ Protect advertisers from offensive environments.
E-learning â†’ Keep online classrooms safe.
News Platforms â†’ Clean article comment sections.

## ğŸ—‚ï¸ Project Structure
project/
â”‚â”€â”€ toxicity_detection.ipynb    # Notebook (model training & evaluation)

â”‚â”€â”€ app.py                      # Streamlit app (interactive UI)

â”‚â”€â”€ train.csv                   # Training dataset

â”‚â”€â”€ test.csv                    # Test dataset

â”‚â”€â”€ model_bilstm.keras          # Saved trained model

â”‚â”€â”€ tokenizer.joblib            # Saved tokenizer

â”‚â”€â”€ meta.joblib                 # Metadata (threshold, config)

â”‚â”€â”€ requirements.txt            # Dependencies

â”‚â”€â”€ README.md                   # Project documentation

# âš™ï¸ Approach
-> EDA & Preprocessing
-> Clean comments (remove URLs, symbols, stopwords).
-> Tokenization & padding.
-> Model Development
-> Train a BiLSTM deep learning model.
-> Evaluate with Precision, Recall, F1-score.
->Save model + tokenizer + metadata.

Streamlit App
Enter a comment â†’ get toxicity prediction.
Upload CSV â†’ get batch predictions.
View probability scores and decisions.

# ğŸ“Š Results

Model trained with good F1-score (balance between precision & recall).
Real-time predictions available via Streamlit.
CSV bulk processing supported.

# ğŸš€ Deployment Guide
Run Locally
pip install -r requirements.txt
streamlit run app.py

App runs at: http://localhost:8501
Deploy Online (Streamlit Cloud)
Push repo to GitHub.

Go to https://share.streamlit.io
.

Select repo â†’ choose app.py.

Deploy â†’ get public app link.

For detailed setup, see Deployment Guide
 (if included).

# ğŸ“¦ Deliverables

âœ… toxicity_detection.ipynb â†’ Notebook (training + evaluation)
âœ… app.py â†’ Streamlit app (real-time + CSV predictions)
âœ… requirements.txt â†’ Dependencies
âœ… README.md â†’ Documentation
âœ… Demo Video â†’ Walkthrough of notebook + app

## ğŸ Conclusion

This project demonstrates how Deep Learning + NLP + Streamlit can create a practical solution for comment toxicity detection, useful for real-world online platforms.


### Model File
Download trained model from Google Drive:
[model_bilstm.keras](https://drive.google.com/file/d/1cM0LTLDqcIEd3RQbP8kZspQBl11lCbLs/view?usp=sharing)



